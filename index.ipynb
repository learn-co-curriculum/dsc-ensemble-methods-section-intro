{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "## Introduction\n",
    "This lesson summarizes the topics we'll be covering in section 35 and why they'll be important to you as a data scientist.\n",
    "\n",
    "## Objectives\n",
    "You will be able to:\n",
    "* Understand and explain what is covered in this section\n",
    "* Understand and explain why the section will help you to become a data scientist\n",
    "\n",
    "## Ensemble Methods\n",
    "\n",
    "The idea of ensemble methods is to bring together multiple models to use them to improve the quality of your predictions when compared to just using a single model. In many real world problems and kaggle competitions, ensemble methods tend to outperform any single model.\n",
    "\n",
    "### Ensemble Methods\n",
    "\n",
    "We start the section by providing an introduction to the concept of ensemble methods, explaining how they take advantage of the delphic technique (or \"wisdom of crowds\") where the average of multiple independent estimates is usually more consistently accurate than the individual estimates.\n",
    "\n",
    "We also provide an introduction to the idea of bagging (Bootstrap AGGregation).\n",
    "\n",
    "### Random Forests\n",
    "\n",
    "We then look at random forests - an ensemble method for decision trees that takes advantage of bagging and the subspace sampling method to create a \"forest\" of decision trees that provides consistently better predictions than any single decision tree.\n",
    "\n",
    "### GridsearchCV\n",
    "\n",
    "In the last section we introduced some of the common hyperparameters for tuning a decision tree. In this lesson we look at how you can use GridSearchCV to perform an exhaustive search across multiple hyperparameters and multiple possible values to come up with a better performing model.\n",
    "\n",
    "### Gradient Boosting and Weak Learners\n",
    "\n",
    "Next up, we introduce the concept of boosting which is at the heart of some of the most powerful ensemble methods such as Adaboost and Gradient Boosted Trees. \n",
    "\n",
    "### XGBoost\n",
    "\n",
    "We then round out the section by introducing XGBoost (eXtreme Gradient Boosting) - the top gradient boosting algorithm currently in use.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Summary\n",
    "\n",
    "You will often find youself using a range of ensemble techniques to improve the performance of your models, so this section will provide you with experience with techniques that will help you to improve the quality of your modeling.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
